\section{Analysis and Discussion}
This section dives into the meaning and deductions that can be derived from the experiments and the results.

Both \textbf{3.1)} and \textbf{3.2) }represent the eventually noticeable exponential growth of both approaches, but in different manners. It is worth mentioning again that these values include the analysis, i.e. the graphs went through the minimal forbidden minor finder, not the generation alone - despite what the title may suggest. That's because the generation does take the majority of the CPU time, but it should not be ignored that the analysis is also time-consuming.  For the CGE, it is possible to understand how unfeasible the program would become if an enumeration of all candidate graphs with 8 vertices or more was to be opted for. For the binomial graph sampling, that also starts to become an issue worth taking into account at around 14 vertices. 

\textbf{3.2.1)} As we can see, for example, for 8 vertices in $F(4)$, by having an edge creation probability of 80\%, there is a 64.43\% chance of automatically generating graphs with starting treewidth 5 (so, 4+1), which is what is relevant to this case. Using an edge creation probability of 50\%, on the other hand, would only have a 2.58\% chance of generating graphs that actually mattered. This characterizes the model that maximizes the chances of having relevant minors within the set of generated graphs.

\textbf{3.2.2)} This plot demonstrates the average time it takes for the program to find any minimal forbidden minor (not necessarily a new one), for different numbers of vertices. It is possible to see through the colourful lateral scale that the difference starts to hint at exponential growth, as it progresses from lightly-coloured low regions to much darker ones in the span of one-unit vertex progression (from 10 to 11).

\textbf{3.2.3)} This proves the difficulty of the sampling method in covering all the existing connected non-isomorphic graphs. Given the sample is, on average, only computable with a size of 2,000,000 generations - it is noticeable the measly attempt at covering all of them. And even within the 100\% coverage instances, there is no guarantee they all are indeed covered, as graphs are randomly generated after all. Despite having found a reasonable number of MFMs, it cannot be denied that the guarantee of exhaustiveness is easy to counter by taking a look at this graph. Most candidate graphs are most likely never even generated.

\textbf{3.3)} This bar plot allows comprehension of the impact of the connectivity pruning on reducing the graph search space pre-analysis. It is quite insignificant for lower numbers of vertices, which makes sense by recalling the optimal parameters model (see 3.2.1). Again, the tendency is the lower the number of vertices, the higher the edge creation probability parameter, in order to achieve the highest ratio of graphs with the desired treewidth, to begin with. Thus, generating with an 80\% chance of creating an edge will naturally result in more connected graphs, as opposed to 42.5\% (which is the value for 12 vertices). Pruning becomes increasingly important as graphs with more vertices are generated.

\textbf{3.4)} Analysis done at every condition took around 724.6 seconds, while analysis that was done solely on the generated graphs themselves took 30.7 seconds, on average. The first method does outperform the second at finding minimal forbidden minors, but not significantly. An average of 0.35 and 0.28 found minimal forbidden minors per 50,000-sized run, respectively. It is true that re-analyzing the graphs at every vertex deletion and at every edge deletion or contraction is an approach that can help when samples are representative (e.g. for 8 vertices, it can be a worthy compromise, as the chances of covering the existing valid graphs are larger), but as there is a need to analyze more samples (to cover the maximum ground), the run-time disadvantage heavily hinders the process, and with a non-significant outperformance, result-wise. It is thus fair to say that it represents a compromise that is not worth opting for.

From analyzing this collection of results, it is possible to gain insights into the overall challenges faced by this program. While several optimizations were conducted throughout, it appears they are of limited use in the grand scheme of things. Finding an exhaustive (or close to exhaustive) list of forbidden minors is a challenging task - individually at the very least!