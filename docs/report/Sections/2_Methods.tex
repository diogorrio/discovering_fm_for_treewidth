\section{Methods}
The methods used within the thesis are of various natures. Some focus on generating or collecting a valid database, others on analyzing them. There were efforts to efficiently and sequentially fill the database with correct findings, as the algorithms ran throughout the semester. In this section, one can take a deeper look into what comprises the system of the project.

\subsection{Graph Generation}
Regardless of the chosen approach, a shared necessity was generating a valid set of graphs that would be fed into the algorithms that discern critical forbidden minors. With that in mind, it became a fair starting point to understand the basic fundamentals of the types of graphs that were to be found. In other words, at the most basic of levels, all (minimal) forbidden minors naturally have a few structural characteristics in common. At the very least, they are always connected, with no vertices without any edge connecting them to the rest of the graph. They are also undirected, which means the sense of direction is irrelevant and therefore unnecessary to model, as are edge weights.

Another important characteristic to monitor is graph isomorphism. There was a need to find a way to guarantee that any minor that was identified could not be rearranged into one same one that had been found previously. There are two ways of doing this: either through a post-analysis concept called isomorphism-checking or by modeling unlabelled graphs from the beginning, which revolves around the idea of not giving a particular ID (or label) to each of the graph vertices. The latter would inherently be a method of isomorphism checking but would limit the amount of graph generation tools that could be used. For that reason, and given the fact that applying restrictions from the conception of the thesis would most likely hinder its development, the option chosen was the former. There is already a significant number of efficient isomorphism-checking tools, which translates into more time focused on the task at hand.

\subsubsection{Combinatorial (Exhaustive) Graph Enumeration}
The combinatorial graph enumeration graph generation approach is a naive one. It consists in exhaustively conceiving all graphs with any provided number of vertices. This approach is designed manually and can therefore be logically built with the fundamental graph characteristics mentioned previously. This means the outcomes of this method would result in having every valid graph, ready for analysis. Exhaustively generating graphs sounds promising - if it weren't for the fact that the number of possible combinations increases exponentially \cite{oeisA001349}. Brute-forcing through the number of vertices is therefore not computationally feasible in the long term.

Nevertheless, the fact that it guarantees exhaustiveness is quite tempting. There is the certainty that no graphs are overlooked. For that reason, this approach is kept up to a reasonable number of vertices, established to be 7, where it significantly outperforms random graph sampling. That is easily explainable, as the number of possible combinations of vertices and edges is small enough to justify the safety of exhaustiveness. With random sampling, the loom of having at least a graph missing is always present.

The combinatorial graph enumeration algorithm was based on Arseny Khakhalin's work \cite{khakhalin2020}. It generates graphs recursively, by including or skipping each edge, using a lexicographical order of construction. Again, it performs well up to 7 vertices. Venturing over that number, however, starts to become seriously challenging, since the computational time increases by $2^{n-1}$ with each vertex increase. An alternative approach was thus worth considering.

\subsubsection{The Package 'nauty': Possibilities and Limitations}
The package 'nauty' is an old library developed in and for C. It is a set of procedures that aims to, among many other functionalities, quickly compute several graph operations, such as isomorphism and canonical labeling. A very useful program for this thesis would have been 'geng', included within 'nauty'. This program gives the user the ability to efficiently (and exhaustively!) enumerate non-isomorphic graphs. By tweaking certain parameters, it was possible to have it generate the types of valid graphs desired - connected and specific to the number of wanted vertices.

Once again, this presented itself as a promising approach (and as it will be suggested later on, it may very well still be). However, a setback surged. The format with which 'nauty' outputs its results is an unusual one, developed with the intent of reducing the size of the files containing them. It is a smart procedure since by using the .g6 file format, any usual graph representations (edge lists, adjacency matrices or lists, ...) can be diminished tremendously, while the program itself remains able to interpret any of those entries. Unfortunately, this distinguished factor limited making consistent use of 'nauty'. The conversion attempts (from .g6 files to ones containing any usable graph representations) were in vain. The Python functions I tried applying were prone to incorrect conversions, and this was the main reason why this approach was dropped and yet another alternative was considered.

\subsubsection{Random Binomial Graph Sampling}
Random sampling: a good solution to many computational intractability roadbloacks. After the somewhat inconsequential previous two methods, random sampling seemed like an interesting change of pace. Until it became an unrealistic effort (i.e. until the sample became measly in comparison with the possibly generatable graphs), it could yield a significant number of results for a number of vertices higher than 7. This was worth following, as it can undoubtedly generate some more valid graphs, which is progress towards finding more forbidden minors after all.

The random graph sampling approach used was based on the Erdős–Rényi model. Essentially, the method generates a set of graphs based on but a few parameters: the number of vertices in the graph to be created, the probability of any two vertices being connected by an edge (i.e., whether an edge is created, independently randomly sampled between every pair of vertices), and lastly the actual number of samples to be generated with these conditions.

Sequentially, this model also filters out any graphs that are not connected, pruning the search space from irrelevant graphs. The isomorphism check could be conducted at this stage, however, given the program was developed to run many times, each time trying to find new elements of $F(k)$, it is more efficient to only perform the isomorphism check on all the graphs that survive the conditions that will be explained in the following sections. That is because iterating through such a possibly large sample, in what is arguably the most time-consuming task of the final program, is not beneficial. The isomorphism checking always needs to be done anyway when graphs are on the verge of being added to the database - so that there is a certainty that any added graphs are indeed \textit{new} minimal forbidden minors.

\subsubsection{The Combined Approach}
In the end, the graph generation approach which was chosen to be the main one was a junction of both the ones developed.

As suggested earlier, the feasibility of guaranteeing exhaustiveness up to 7 vertices is something worth chasing. For that reason, the combined approach was settled using combinatorial graph enumeration up to that threshold, and random sampling thereon.

\subsection{Forbidden Minor Finding}
After there is a set of graphs to work with, it is necessary to perform the actual analysis of it. Minimal forbidden minors are thus identified through a method that logically applies the conditions of what it means 'to be minimal'.

\subsubsection{Simplifying the Forbidden Minor Finding through Conditioning}
Without having to dive too profoundly into the theory of forbidden minors, it is possible to discern how a sub-graph can be minimal. Since the forbidden minors at focus in this thesis are related to the treewidth of a graph, the observable change in the treewidth itself, as we apply vertex/edge deletion and edge contraction, is sufficient to determine the \textit{minimality} of a minor.

All in all, what the method does is apply the three basic conditions of what it means to be a graph minor - all while constantly checking for any treewidth alterations that may originate from it. Recalling, a minor is the resulting sub-graph $G$ obtainable from an original graph $H$ by subjecting $H$ to operations such as deleting a vertex, deleting an edge, or contracting an edge, for each instance of a vertex or an edge. If the treewidth upon doing so does \textit{not} decrease, then one can be sure that \textit{minimality} is yet to be achieved.

In practical terms, what all this theory means is the following: 

For each generated graph $G$, $tw(G)$ is initially checked. If its treewidth is different from the established treewidth reference (which is always $k+1$), $G$ is discarded. Discarding a graph means that the graph cannot possibly be a minimal forbidden minor. These three upcoming steps can be done in arbitrary order. Next, for each vertex in the graph, that vertex is removed. $tw(G)$ is checked again. If there exists a vertex $v$ such that deleting $v$ does not cause the treewidth to decrease, the graph is dropped. The same exact process is also done for each edge in the graph, where that edge is removed. Finally, the same process is yet again conducted for each edge in the graph, this time contracting it. Surviving all these conditions represents the finding of a minimal forbidden minor. Whether a new one or not, that is to be determined at a later stage. 

Any discarded graphs are abandoned and not further looked into, as not doing so (i.e. not abandoning them) would heavily impact the run-time and limit the sample size that could realistically be analyzed. Long term, it is heavily inefficient to perform analysis as we deconstruct a graph, as the samples are not representative of all the graphs that exist. The experiment conducted on this further corroborates this decision (see 3.4).

Regardless, a necessary aspect for this method to work is, naturally, having an exact (and not purely heuristic) treewidth solver.

\subsubsection{\textit{QuickBB}: The Exact Treewidth Solver}
With the goal of knowing the true treewidth of a graph in mind, the program has the $QuickBB$ algorithm \cite{gogate2004} incorporated within it. It is a Branch and Bound optimization algorithm that computes the treewidth of any given undirected graph. It works by performing a search in the perfect elimination ordering of graph vertices space, using theory-based pruning and propagation techniques. An important characteristic that is behind the choice of this algorithm is the fact that it improves upon alternative methods, such as $QuickTree$, which is a complete algorithm \cite{gogate2004}. 

Advantages such as its good anytime performance, resulting in unprecedentedly accurate upper bounding on graphs whose optimal treewidth had not been feasible until the development of $QuickBB$, contributed heavily towards adopting (and adapting) this method. Nowadays, however, it is true that $QuickBB$ is outperformed by alternative C++ or Java solvers, such as $BestTW$ or $BFHT$ \cite{dow2007}. The reason behind using $QuickBB$ despite the known outperformers is that newer solvers are not readily available in Python, which was the language used in this thesis. Besides, the computed treewidths in this project were small, after all - which removes the need for current state-of-the-art solvers.

\subsubsection{Isomorphism Checking}
Another important aspect of assuring the preciseness and validity of the database is isomorphism checking. Formally, two graphs $G$ and $H$ are isomorphic if there exists a bijection between their vertex sets in a way that for any pair of vertices $v_1$ and $v_2$ in $G$, there exists a corresponding pair of vertices $u_1$ and $u_2$ in $H$ (and vice-versa), such that the presence or absence of edges is preserved. Essentially, shared graph morphologies, with different labels. Having hidden repeated entries is an endangering issue, as it would contribute towards getting the wrong feel and number of results. 

Therefore, firstly, the program - upon being on the verge of adding a minimal forbidden minor into the database - collects all the already existing instances from the specific table it is meant to add into. This collected set then refers to the set of identified forbidden minors in the current program run and performs a one-on-one comparison between every element of both sets, using the $vf2$ algorithm, which performs well on large graphs \cite{foggia2001}. This is thus the final condition separating a found forbidden minor from being added into the database. If it does happen to be a newfound minimal forbidden minor, structurally different from all its peers, it progresses onto the database, finishing the entire forbidden minor finding process.

\subsection{The Idea of Designing a Database}
The designed program is not single-run. What this means is that in order to find a reasonable number of forbidden minors, the program is intended to run multiple times, because otherwise, it would be computationally impossible to achieve results. In answer to this, a database proved to be an interesting approach, since it can be updated and published externally through various means. Any issues that the program faces are independent of the database, keeping it a safe method to store any outcomes. This is called data persistence, which is a vital advantage in computationally intensive programs such as this one.

The fact that instances can be easily distinguishable from one another, by categorizing graphs with additional parameters also helps with the handling of the data. The versatility and scalability advantages that come with a database are of such importance that in truth an alternative option was never seriously considered.

Yet, this is not all. The familiarity with MySQL and the uncertainties at the beginning of how the findings would be communicated in the end were equally major influences.
